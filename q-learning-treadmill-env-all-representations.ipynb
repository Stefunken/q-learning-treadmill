{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "\n",
    "\n",
    "The library [gym](https://gym.openai.com/) is an open-source library, which gives you access to a standardized set of environments to simulate different RL algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "\n",
    "############################################################################################\n",
    "# we study 3 possible internal representations, the index of the representation is defined at the beginning of the notebook, inthe seceon cell\n",
    "internal_representation = 2 # 0=time index, 1=position and time index, 2=position index and rear touching (0,1) \n",
    "\n",
    "if internal_representation == 0:\n",
    "    results_folder = 'Results-Front-0.5-TimeIndex-' + str(now)[:16]\n",
    "elif internal_representation == 1:\n",
    "    results_folder = 'Results-Front-0.5-TimePositionIndex-' + str(now)[:16]\n",
    "else: \n",
    "    results_folder = 'Results-Front-0.5-PositionIndexRearWall-' + str(now)[:16]\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "os.mkdir(results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-Environment interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define discrete dynamics of the environment and reward schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to sample from a discrete distribution, for the initial state distribution\n",
    "def categorical_sample(prob_n, np_random): \n",
    "    \"\"\"\n",
    "    Sample from categorical distribution\n",
    "    Each row specifies class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np_random.rand()).argmax()\n",
    "\n",
    "# the class DiscreteEnv slightly modified\n",
    "class DiscreteEnv_v2(Env):\n",
    "\n",
    "    \"\"\"\n",
    "    Has the following members\n",
    "    - nS: number of states\n",
    "    - nA: number of actions\n",
    "    - P: transitions (*)\n",
    "    - isd: initial state distribution (**)\n",
    "    (*) dictionary dict of dicts of lists, where\n",
    "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
    "    (**) list or array of length nS\n",
    "    \"\"\"\n",
    "    def __init__(self, nS, nA, P, isd):\n",
    "        self.P = P\n",
    "        self.isd = isd\n",
    "        self.lastaction=None # for rendering\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        seed = 0\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        seed = 0\n",
    "\n",
    "        return [seed]\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        self.s = categorical_sample(self.isd, self.np_random)\n",
    "        self.lastaction = None\n",
    "        if self.s == 0:            \n",
    "            self.enterone = False\n",
    "        else:\n",
    "            self.enterone = True\n",
    "            \n",
    "        return self.s, self.enterone\n",
    "        \n",
    "    def step(self, a, time,t_min,t_max,t_exit):      \n",
    "        transitions = self.P[self.s][a]\n",
    "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
    "        p, s  = transitions[i]\n",
    "        self.s = s # the new state becomes the actual one \n",
    "        self.enterone = self.update_enterone()        \n",
    "        \n",
    "        #print(self)\n",
    "        #print(time)\n",
    "        \n",
    "        r, d = self.generate_reward(time,t_min,t_max,t_exit)\n",
    "        #self.lastaction = a\n",
    "        return (s, r, d, {\"prob\" : p})\n",
    "        #return (s, {\"prob\" : p})\n",
    "           \n",
    "        \n",
    "           \n",
    "    def generate_reward(self, time,t_min,t_max,t_exit):    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if (time==t_exit and self.enterone==False): # being away of s=0 (reward zone) maximum at t_exit+1 \n",
    "        \n",
    "            reward = -1\n",
    "            end_trial = True\n",
    "        \n",
    "        if (self.s == 0 or self.s == int(self.nS/2)): \n",
    "\n",
    "            if (time<t_min):\n",
    "                \n",
    "                reward = -1\n",
    "                end_trial = True\n",
    "            \n",
    "            if (time>=t_min and time<t_max):\n",
    "                r=[3 - 3/(time_max-time_min)*i for i in range(int(time_max-time_min))]\n",
    "                    \n",
    "                reward = r[int(time-time_min)]\n",
    "                end_trial = True\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            if time < time_max-1:\n",
    "                reward = -.1 #-.1\n",
    "                end_trial = False\n",
    "          \n",
    "            \n",
    "            if time==t_max-1: # this can only be time == t_max if in the trial loop time<=time_max            \n",
    "                reward = -.5\n",
    "                end_trial = True\n",
    "        \n",
    "\n",
    "    \n",
    "        return reward, end_trial\n",
    "    \n",
    "    \n",
    "    def update_enterone(self):\n",
    "        #time += 1\n",
    "        \n",
    "        if (self.enterone == False):             \n",
    "            if self.s == 0:                \n",
    "                self.enterone = False        \n",
    "            else:            \n",
    "                self.enterone = True        \n",
    "        return self.enterone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r=[3 - 3/(time_max-time_min)*i for i in range(int(time_max-time_min))]                   \n",
    "#r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTreadmillEnv(DiscreteEnv_v2):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        nS = 10 # must be an even number\n",
    "        nA = 3\n",
    "        \n",
    "        #isd = np.zeros(nS)\n",
    "        #isd[0] = 1 # trials always begin with the rat at the front of the treadmill\n",
    "        \n",
    "        alpha = 0.5\n",
    "        isd = [alpha*(1-alpha)**i for i in range(int(nS/2))]\n",
    "        isd = isd/np.sum(isd)                                         \n",
    "        \n",
    "        #alpha = [.9,.2]\n",
    "        #isd = [alpha[0]*(1-alpha[0])**i + alpha[1]*(1-alpha[1])**(int(nS/2)-1-i)  for i in range(int(nS/2))]       \n",
    "        #isd = isd/np.sum(isd)\n",
    "        \n",
    "        \n",
    "        \n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "        for s in range(nS):\n",
    "            for a in range(nA):\n",
    "                if a == 0:\n",
    "                    if s == int(nS/2)-1 or s == nS-1:\n",
    "                        newstate = nS-1 # cannot enter the rear wall\n",
    "\n",
    "                    else:   \n",
    "                        newstate = s+1            \n",
    "            \n",
    "                elif a == 1:\n",
    "                \n",
    "                    if s == int(nS/2)-1:                    \n",
    "                        newstate = nS-1\n",
    "                                                           \n",
    "                    else:\n",
    "                        newstate = s\n",
    "            \n",
    "                else:\n",
    "                    if s == 0 or s == int(nS/2):\n",
    "                        newstate = s\n",
    "                    elif s == int(nS/2)-1:\n",
    "                        newstate = nS-2                       \n",
    "                    else:\n",
    "                        newstate = s-1\n",
    "                    \n",
    "                P[s][a].append((1.0,newstate))\n",
    "        DiscreteEnv_v2.__init__(self, nS, nA, P, isd)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the internal state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_internal_state(internal_representation,time,state,n_positions,pos_ind):\n",
    "\n",
    "    if internal_representation == 0:\n",
    "        internal_state = time # the internal state is the time index\n",
    "    elif internal_representation == 1:\n",
    "        internal_state = (time)*n_positions + pos_ind[state] # the internal state is the position and time index\n",
    "    else: \n",
    "        internal_state = state # the internal state is the position index and touching the rear (0,1)\n",
    "\n",
    "    return internal_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define epsilon greedy and softmax policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q,i_current,epsilon=.1):\n",
    "    \n",
    "    if np.random.random()<=epsilon  or np.sum(Q[i_current,:]==0):\n",
    "        a_current = np.random.randint(0,Q.shape[1])      \n",
    "    else:\n",
    "        a_current = np.argmax(Q[i_current,:])\n",
    "        \n",
    "    return a_current \n",
    "\n",
    "###############################################################\n",
    "def softmax(x,beta): # this is just the softmax distribution\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(beta*(x - np.max(x)))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def softmax_policy(Q,s_current,p_current,nS,beta = 3): # nS is the number of positions\n",
    "    \n",
    "    Q_current = Q[s_current,:]\n",
    "    action_prob = softmax(Q_current,beta) # probability of each action, Q_current = Q[i_current,:]\n",
    "    \n",
    "    # this is a constraint related to the environment\n",
    "    ###################################################\n",
    "    if (p_current == 0):\n",
    "        action_prob[2] = 0\n",
    "        action_prob = action_prob/np.sum(action_prob)\n",
    "    elif (p_current == int(nS/2)-1):\n",
    "        action_prob[0] = 0\n",
    "        action_prob = action_prob/np.sum(action_prob)        \n",
    "    else:\n",
    "        action_prob = action_prob\n",
    "    ###################################################\n",
    "    \n",
    "    prob_n = np.asarray(action_prob)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    a_current = (csprob_n > np.random.rand()).argmax()\n",
    "        \n",
    "    return a_current\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q learning-update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_update(i_current,a_current,rew,i_next,Q,end_trial,alpha=.1,gamma=.98):\n",
    "    \n",
    "    if end_trial == 0:\n",
    "        delta = rew + gamma*np.max(Q[i_next,:]) - Q[i_current,a_current] # td rule for Q-learning\n",
    "    else:\n",
    "        delta = rew + gamma*0 - Q[i_current,a_current] # td rule for Q-learning, value of exit state is 0\n",
    "\n",
    "    Q[i_current,a_current] += alpha*delta\n",
    "    \n",
    "    return Q, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for final plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_panel_caption(axes: tuple, offsetX: tuple, offsetY: tuple, **kwargs):\n",
    "    \"\"\"\n",
    "    This function adds letter captions (a,b,c,d) to Axes in axes\n",
    "    at top left, with the specified offset, in RELATIVE figure coordinates\n",
    "    \"\"\"\n",
    "    assert len(axes)==len(offsetX)==len(offsetY), 'Bad input!'\n",
    "    \n",
    "    fig=axes[0].get_figure()\n",
    "    fbox=fig.bbox\n",
    "    for ax,dx,dy,s in zip(axes,offsetX,offsetY,string.ascii_lowercase):\n",
    "        axbox=ax.get_window_extent()\n",
    "    \n",
    "        ax.text(x=(axbox.x0/fbox.xmax)-abs(dx), y=(axbox.y1/fbox.ymax)+abs(dy),\n",
    "                s=s,fontweight='extra bold', fontsize=10, ha='left', va='center',\n",
    "               transform=fig.transFigure,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(ax,variable_to_plot,goalTime,seed_vector,n_sessions,TaskParamToPlot,color='k'):\n",
    "\n",
    "    #goalTime=7\n",
    "    \n",
    "    x=np.arange(n_sessions)+1\n",
    "    data=variable_to_plot\n",
    "    y=np.nanpercentile(data,50,axis=0)\n",
    "    yerr=np.nanpercentile(data,(25,75),axis=0)\n",
    "    ax.errorbar(x,y,yerr=abs(yerr-y), ecolor=color, fmt='-o',color=color,\n",
    "                elinewidth=1, markersize=4, markerfacecolor='w',zorder=3)\n",
    "    \n",
    "    \n",
    "    if TaskParamToPlot==\"ET\":\n",
    "        ax.hlines(y=goalTime, xmin=x[0], xmax=x[-1], linestyle='--', lw=1, color='m', zorder=1)\n",
    "        ax.text(x=x[0], y=10, s=f'$n={len(seed_vector)}$ agents', fontsize='xx-small', zorder=5)\n",
    "        ax.set_ylim([0,12])\n",
    "        ax.set_yticks([0,3.5,7,10.5])\n",
    "        ax.set_yticklabels([0,'',7,''])\n",
    "\n",
    "\n",
    "    \n",
    "    ax.set_xlim([x[0]-1,x[-1]+1])\n",
    "    xtick=[1]\n",
    "    for i in range(1,n_sessions+1):\n",
    "        if i%5==0:\n",
    "            xtick.append(i)\n",
    "    ax.set_xticks(xtick)\n",
    "    ax.spines['bottom'].set_bounds(x[0],x[-1])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "    #ax.set_xlabel('Session#')\n",
    "    #ax.set_ylabel(TaskParamToPlot)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agent_ET(ax, med, highError, lowError,goalTime,n_sessions,color='k'):\n",
    "    \n",
    "    \n",
    "    x = [i+1 for i in range(n_sessions)]\n",
    "    \n",
    "    ax.errorbar(x,med,yerr=np.vstack([med-lowError,highError-med]),\n",
    "                ecolor=color, fmt='-o',color=color,elinewidth=1, markersize=4,\n",
    "                markerfacecolor='w',zorder=3)\n",
    "\n",
    "\n",
    "    ax.hlines(y=goalTime, xmin=x[0], xmax=x[-1], linestyle='--', lw=1, color='m', zorder=1)\n",
    "    ax.set_ylim([0,12])\n",
    "    ax.set_yticks([0,3.5,7,10.5])\n",
    "    ax.set_yticklabels([0,'',7,''])\n",
    "\n",
    "\n",
    "    \n",
    "    ax.set_xlim([x[0]-1,x[-1]+1])\n",
    "    xtick=[1]\n",
    "    for i in range(1,n_sessions+1):\n",
    "        if i%5==0:\n",
    "            xtick.append(i)\n",
    "    ax.set_xticks(xtick)\n",
    "    ax.spines['bottom'].set_bounds(x[0],x[-1])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "    ax.set_xlabel('Session#')\n",
    "    ax.text(x=x[-1], y=10, s='seed #' + str(30), zorder=5,ha='right',fontsize='xx-small')\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agent_SD(ax, var_to_plot, seed_number,n_sessions,color='olive'):\n",
    "\n",
    "    data=var_to_plot\n",
    "    x=np.arange(len(data))+1\n",
    "    ax.plot(x,data, color=color, marker='o',linestyle='-',linewidth=1, markersize=4, markerfacecolor='w',zorder=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_distributions(fig, gs,p_TR_correct,p_TR_error,p_TR_omission,cmapCorrect,cmapError,cmapOmission,selected_sessions,alpha=0.5):\n",
    "    \n",
    "    n_states = int(2*p_TR_correct.shape[0])\n",
    "    pos_ind = np.concatenate(([i for i in range(int(n_states/2))],[i for i in range(int(n_states/2))]),axis=0)\n",
    "\n",
    "    axes=[]\n",
    "    for i_p, i_s in enumerate(selected_sessions):\n",
    "    \n",
    "        ax=fig.add_subplot(gs[i_p])\n",
    "        axes.append(ax)\n",
    "    \n",
    "        ax.pcolor(p_TR_correct[:,:,i_s], edgecolors='none', linewidths=1, cmap=cmapCorrect, vmin=0.0, vmax=1.0,alpha=alpha)\n",
    "\n",
    "        ax.pcolor(p_TR_error[:,:,i_s], edgecolors='none', linewidths=1, cmap=cmapError, vmin=0.0, vmax=1.0,alpha=alpha)\n",
    "        \n",
    "        ax.pcolor(p_TR_omission[:,:,i_s], edgecolors='none', linewidths=1, cmap=cmapOmission, vmin=0.0, vmax=1.0,alpha=alpha)\n",
    "\n",
    "                \n",
    "        for ind_t in range(sessions[i_s],sessions[i_s+1]):\n",
    "            l = len(pos_ind[sequence_all_states[ind_t]])\n",
    "            \n",
    "            if correct_all[ind_t] == True:\n",
    "                color_plot = 'Olive'\n",
    "            elif correct_all[ind_t] == False : \n",
    "                color_plot = 'Tomato'\n",
    "            else :\n",
    "                color_plot = 'gray'\n",
    "                        \n",
    "            ax.plot([i+.5 for i in range(l)],pos_ind[sequence_all_states[ind_t]]+.5, '-',\n",
    "                    color = color_plot, linewidth=.8,alpha=.02)\n",
    "        \n",
    "        ax.set_xticks([i+0.5 for i in range(0,20,5)])\n",
    "        ax.set_xticklabels([str(i) for i in range(0,20,5)])\n",
    "    \n",
    "        ax.set_yticks([i+0.5 for i in range(5)])\n",
    "        if i_p == 0:\n",
    "            ax.set_yticklabels([str(i) for i in range(5)])\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    \n",
    "    \n",
    "        ax.set_xlim(0,16)\n",
    "\n",
    "    \n",
    "        ax.set_title('Session #' + str(i_s+1),fontsize=10)\n",
    "        if i_p == 0:            \n",
    "            ax.set_ylabel('Position index')                    \n",
    "        ax.set_xlabel('Time index')\n",
    "        \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "    return axes\n",
    "##########################################################33\n",
    "\n",
    "def add_legend_trajectory(ax):\n",
    "    err_marker = matplotlib.lines.Line2D([0],[0],color='Tomato',label='Error')\n",
    "\n",
    "    corr_marker = matplotlib.lines.Line2D([0], [0],color='Lime',label='Correct')\n",
    "    \n",
    "    omis_marker = matplotlib.lines.Line2D([0], [0], color='Gray',label='Omission')\n",
    "\n",
    "\n",
    "    leg=ax.legend(handles=[err_marker,corr_marker,omis_marker],loc=(.4,.55),labelspacing=0.2, ncol=1, fontsize=6, framealpha=0)\n",
    "    \n",
    "    return leg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_distributions_separated(fig, gs,p_TR_correct,p_TR_error,cmapCorrect,cmapError,selected_sessions,alpha=1):\n",
    "    \n",
    "    axes=[]\n",
    "    for i_p, i_s in enumerate(selected_sessions):\n",
    "    #############################################\n",
    "    # Correct trials\n",
    "        ax=fig.add_subplot(gs[i_p])\n",
    "        axes.append(ax)   \n",
    "        ax.pcolor(p_TR_correct[:,:,i_s], edgecolors='none', linewidths=1, cmap=cmapLime, vmin=0.0, vmax=1.0,alpha=alpha)\n",
    "\n",
    "        ax.set_xticks([i+0.5 for i in range(0,20,5)])\n",
    "        ax.set_xticklabels([str(i) for i in range(0,20,5)])\n",
    "    \n",
    "        ax.set_yticks([i+0.5 for i in range(5)])\n",
    "        if i_p == 0:\n",
    "            ax.set_yticklabels([str(i) for i in range(5)])\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    \n",
    "    \n",
    "        ax.set_xlim(0,16)\n",
    "\n",
    "    \n",
    "        ax.set_title('Session #' + str(i_s+1),fontsize=10)\n",
    "        #ax.set_xlabel('Time index')\n",
    "        \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "    #############################################\n",
    "    # Error trials        \n",
    "        ax=fig.add_subplot(gs[i_p+len(selected_sessions)])\n",
    "        axes.append(ax) \n",
    "        ax.pcolor(p_TR_error[:,:,i_s], edgecolors='none', linewidths=1, cmap=cmapTomato, vmin=0.0, vmax=1.0,alpha=alpha)\n",
    "        \n",
    "        ax.set_xticks([i+0.5 for i in range(0,20,5)])\n",
    "        ax.set_xticklabels([str(i) for i in range(0,20,5)])\n",
    "    \n",
    "        ax.set_yticks([i+0.5 for i in range(5)])\n",
    "        if i_p == 0:\n",
    "            ax.set_yticklabels([str(i) for i in range(5)])\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    \n",
    "    \n",
    "        ax.set_xlim(0,16)\n",
    "                 \n",
    "        ax.set_xlabel('Time index')\n",
    "        \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return axes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entrance_times(fig, gs,sessions,selected_sessions,correct_all,entrance_times,goalTime):\n",
    "\n",
    "\n",
    "    axes=[]\n",
    "    for i_p, i_s in enumerate(selected_sessions):\n",
    "    \n",
    "        ax=fig.add_subplot(gs[i_p])\n",
    "        axes.append(ax)\n",
    "        \n",
    "        mask = np.asarray(correct_all[sessions[i_s]:sessions[i_s+1]-1])\n",
    "        times = np.asarray(entrance_times[sessions[i_s]:sessions[i_s+1]-1])\n",
    "        ax.plot(entrance_times[sessions[i_s]:sessions[i_s+1]-1],linewidth=.5,alpha=.2)\n",
    "        ax.plot(np.where(mask==False)[0],times[mask==False],'.',markersize=4,color='Tomato')\n",
    "        ax.plot(np.where(mask==True)[0],times[mask==True],'.',markersize=4,color='Lime')\n",
    "        ax.plot(np.where(mask==None)[0],times[mask==None],'.',markersize=4,color='gray')\n",
    "\n",
    "        ax.plot([1,100],[goalTime,goalTime],'--', color='m',linewidth=1.5)\n",
    "        \n",
    "        ax.set_xlim(0,100)\n",
    "        ax.set_ylim(0,16)\n",
    "    \n",
    "        if i_p == 0:\n",
    "            ax.set_yticklabels([str(i) for i in [0,7,15]])\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "    \n",
    "        ax.set_yticks([i for i in [0,7,15]])\n",
    "\n",
    "        #ax.set_title('Session #' + str(i_s+1), fontsize=10)    \n",
    "        if i_p == 0:         \n",
    "            ax.set_ylabel('Entrance time',fontsize=10)\n",
    "    \n",
    "            \n",
    "        ax.set_xlabel('Trial #',fontsize=10)    \n",
    "            \n",
    "        \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False) \n",
    "    \n",
    "\n",
    "    return axes\n",
    "####################################################3\n",
    "# For the legend\n",
    "def add_legend_entrance_times(ax):\n",
    "    err_marker = matplotlib.lines.Line2D([], [], color=[0,0,0,0],\n",
    "                                       markeredgecolor='Tomato', marker='.',\n",
    "                                       markerfacecolor='Tomato', markersize=4, label='Error')\n",
    "\n",
    "    corr_marker = matplotlib.lines.Line2D([], [], color=[0,0,0,0],\n",
    "                                       markeredgecolor='Lime', marker='.',\n",
    "                                       markerfacecolor='Lime', markersize=4, label='Correct')\n",
    "    \n",
    "    omis_marker = matplotlib.lines.Line2D([], [], color=[0,0,0,0],\n",
    "                                       markeredgecolor='gray', marker='.',\n",
    "                                       markerfacecolor='gray', markersize=4, label='Omission')\n",
    "    leg=ax.legend(handles=[err_marker,corr_marker,omis_marker],loc=(.01,.65),labelspacing=0.2, ncol=1, fontsize=6, framealpha=0)\n",
    "\n",
    "    return leg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop in trials to produce learning by trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(0) # fix the seed for reproducibility\n",
    "\n",
    "time_min = 7 # minimum crossing is at t_min+1\n",
    "time_max = 15 # crossing is possible until t_max, otherwise omission trials at t_max\n",
    "time_exit = 2 # be out of s=0 at time time_exit+1 (in case intial state is s=0)\n",
    "\n",
    "\n",
    "env = SimpleTreadmillEnv()\n",
    "\n",
    "n_actions = env.nA\n",
    "n_positions = int(env.nS/2)\n",
    "n_times = time_max + 1\n",
    "\n",
    "# WARNING:\n",
    "##############################################\n",
    "# we study 3 possible internal representations, the index of the representation is defined at the beginning of the notebook, in the seceon cell\n",
    "\n",
    "#internal_representation = 1\n",
    "\n",
    "if internal_representation == 0:\n",
    "    n_states = n_times\n",
    "elif internal_representation == 1:\n",
    "    n_states = n_times*n_positions\n",
    "else: \n",
    "    n_states = 2*n_positions\n",
    "##############################################\n",
    "\n",
    "pos_ind = np.concatenate(([i for i in range(n_positions)],[i for i in range(n_positions)]),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "greedy = .1\n",
    "lr = .01 # learning rate\n",
    "df = .99 # discount factor\n",
    "\n",
    "\n",
    "n_trials = 3000\n",
    "tps = 100 # trials per sessions, number of trials in each session\n",
    "n_sessions = int(n_trials/tps)\n",
    "\n",
    "sessions = [i for i in range(0,n_trials + tps,tps)]\n",
    "#seed_vector = [i for i in range(2,32,2)]\n",
    "seed_vector = [30 for i in range(15)]\n",
    "#seed_vector = [30]\n",
    "\n",
    "r_max = 3 # these parañeters are defined in the environment \n",
    "cost = -.1\n",
    "epsilon_rews = 1/tps*(r_max+cost*n_times)\n",
    "\n",
    "aux_invT = [4,3,2.5,2,0]\n",
    "aux_invT_step = [.4,.35,.3]\n",
    "invT_vect, invT_step_vect = np.meshgrid(aux_invT, aux_invT_step)\n",
    "invT_vect = invT_vect.flatten('F')\n",
    "invT_step_vect = invT_step_vect.flatten('F')\n",
    "\n",
    "\n",
    "all_times_median = np.zeros((len(seed_vector),n_sessions))\n",
    "all_times_std = np.zeros((len(seed_vector),n_sessions))\n",
    "\n",
    "\n",
    "for ind_agent, ind_seed in enumerate(seed_vector):\n",
    "    \n",
    "    seed_num = ind_seed\n",
    "    np.random.seed(seed_num) \n",
    "    \n",
    "    \n",
    "    i_Q = 0\n",
    "    invT = invT_vect[ind_agent]\n",
    "    invTstep = invT_step_vect[ind_agent]\n",
    "    \n",
    "    sequence_all_states = []\n",
    "    sequence_all_actions = []\n",
    "    sequence_all_rewards = []\n",
    "    sequence_all_endtrial = []\n",
    "    sequence_all_totalreward = []\n",
    "\n",
    "    Q_all= []\n",
    "    correct_all = []\n",
    "    entrance_times = []\n",
    "    initial_positions = []\n",
    "\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    Q_all = np.zeros((n_states,n_actions,n_sessions)) \n",
    "\n",
    "\n",
    "    TR_correct = np.zeros((n_positions,n_times,n_sessions))\n",
    "    TR_error = np.zeros((n_positions,n_times,n_sessions))\n",
    "    TR_omission = np.zeros((n_positions,n_times,n_sessions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i_trial in range(n_trials):\n",
    "    \n",
    "##################################    \n",
    "        if i_trial!= 0 and i_trial%tps == 0:\n",
    "            Q_all[:,:,i_Q] = Q\n",
    "            i_Q +=1\n",
    "            if invT<=10:\n",
    "                invT+= invTstep\n",
    "            \n",
    "            '''\n",
    "            if i_Q == 0:\n",
    "                rews_current = np.sum(sequence_all_totalreward[sessions[i_Q]:sessions[i_Q+1]-1])\n",
    "                diff_rewards = rews_current           \n",
    "            else:\n",
    "                rews_previous = rews_current\n",
    "                rews_current = np.sum(sequence_all_totalreward[sessions[i_Q]:sessions[i_Q+1]-1])\n",
    "                diff_rewards = rews_current - rews_previous\n",
    "            \n",
    "            i_Q  +=1\n",
    "            \n",
    "            \n",
    "            if diff_rewards>epsilon_rews:    \n",
    "                invT += invTstep\n",
    "            else:\n",
    "                invT -= invTstep/3\n",
    "                \n",
    "            if invT <0:\n",
    "                invT = 0\n",
    "            \n",
    "            if invT>10:\n",
    "                invT = 10\n",
    "            '''    \n",
    "###############################################\n",
    "\n",
    "\n",
    "    \n",
    "        TR = np.zeros((n_positions,n_times))\n",
    "\n",
    "        # reset state at the beginning of the trial\n",
    "        s_current, enter_one = env.reset()\n",
    "        end_trial = False\n",
    "        \n",
    "        sequence_trial_states = []\n",
    "        sequence_trial_actions = [] # \n",
    "        sequence_trial_rewards = [None]\n",
    "        sequence_trial_enterone = [enter_one]\n",
    "        sequence_trial_endtrial = [end_trial]\n",
    "        \n",
    "        correct = []\n",
    "    \n",
    "\n",
    "    \n",
    "        initial_positions.append(s_current)\n",
    "\n",
    "\n",
    "        i_time = 0    \n",
    "    \n",
    "\n",
    "        while i_time < time_max:\n",
    "        \n",
    "            \n",
    "            p_current = pos_ind[s_current]\n",
    "            TR[p_current,i_time] +=1\n",
    "            \n",
    "            iS_current = get_internal_state(internal_representation,i_time,s_current,n_positions,pos_ind)\n",
    "            \n",
    "        \n",
    "            Q_current = Q[iS_current,:]        \n",
    "\n",
    "            #a_current = epsilon_greedy_policy(Q,s_current,epsilon=greedy) \n",
    "            a_current = softmax_policy(Q,iS_current,p_current,2*n_positions,beta = invT)        \n",
    "            s_next,rew,end_trial, _ = env.step(a_current,i_time,time_min,time_max,time_exit)\n",
    "            \n",
    "            iS_next = get_internal_state(internal_representation,i_time+1,s_next,n_positions,pos_ind)\n",
    "       \n",
    "        \n",
    "            Q, delta = Q_learning_update(iS_current,a_current,rew,iS_next,Q,end_trial,alpha=lr,gamma=df)   \n",
    "               \n",
    "            sequence_trial_states.append(s_current) # i_time \n",
    "            sequence_trial_actions.append(a_current)\n",
    "            sequence_trial_rewards.append(rew) # i_time +1\n",
    "            sequence_trial_endtrial.append(end_trial)\n",
    "\n",
    "        \n",
    "            if end_trial==True:\n",
    "                TR[pos_ind[s_next],i_time+1] +=1\n",
    "\n",
    "            if end_trial == True:\n",
    "            \n",
    "                if (i_time < time_min):\n",
    "                    correct = False\n",
    "                    TR_error[:,:,i_Q] += TR\n",
    "                elif (i_time >= time_min and i_time < time_max) and rew>0:\n",
    "                    correct = True\n",
    "                    TR_correct[:,:,i_Q] += TR\n",
    "                else: # i_time==time_max    \n",
    "                    correct = None     \n",
    "                    TR_omission[:,:,i_Q] += TR\n",
    "            \n",
    "   \n",
    "        \n",
    "            if end_trial == True:\n",
    "                sequence_trial_states.append(s_next)\n",
    "                sequence_trial_actions.append(None)\n",
    "                break\n",
    "        \n",
    "        \n",
    "            i_time += 1\n",
    "            s_current = s_next\n",
    "\n",
    " \n",
    "    \n",
    "        sequence_all_states.append(sequence_trial_states)\n",
    "        sequence_all_actions.append(sequence_trial_actions)\n",
    "        sequence_all_rewards.append(sequence_trial_rewards)\n",
    "        sequence_all_totalreward.append(np.sum(sequence_trial_rewards[1:]))\n",
    "        sequence_all_endtrial.append(sequence_trial_endtrial)\n",
    "\n",
    "        correct_all.append(correct)\n",
    "        entrance_times.append(i_time+1)\n",
    "\n",
    "        \n",
    "    for i_s in range(len(sessions)-1):\n",
    "        times = [x for x in entrance_times[sessions[i_s]:sessions[i_s+1]] if x is not 15]\n",
    "        all_times_median[ind_agent,i_s] = np.median(times,axis=0)\n",
    "        all_times_std[ind_agent,i_s] = np.std(times,axis=0)\n",
    "        \n",
    "    %run plot-all-the-results.ipynb     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS FOR A 2 COLS FIGURE\n",
    "\n",
    "plt.close('all')\n",
    "figsize=(7,7)\n",
    "fig=plt.figure(figsize=figsize,dpi=600)\n",
    "\n",
    "################################################################################\n",
    "#blank space for the schematic of the model\n",
    "gs0 = fig.add_gridspec(nrows=1, ncols=1, left=0.02, bottom=0.58, right=0.48, top=.98)\n",
    "ax0= fig.add_subplot(gs0[0])\n",
    "ax0.axis('off')\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 1L: One seed ET and SD progrression\n",
    "gs1= fig.add_gridspec(nrows=1, ncols=1, left=0.6, bottom=0.83, right=0.98, top=.98)\n",
    "ax1= fig.add_subplot(gs1[0])\n",
    "plot_agent_ET(ax1, med, highErr, lowErr,goalTime,n_sessions,color='k')\n",
    "\n",
    "ax1_ = ax1.twinx()\n",
    "plot_agent_SD(ax1_, std_ET, seed_number,n_sessions,color=colorTwin1)    \n",
    "\n",
    "\n",
    "ax1_.set_ylim([0,5])\n",
    "ax1_.set_yticks([0,2,4])\n",
    "ax1_.set_yticklabels([0,2,4])\n",
    "ax1_.xaxis.set_visible(False)\n",
    "ax1_.spines['bottom'].set_visible(False)\n",
    "ax1_.spines['top'].set_visible(False)\n",
    "ax1_.spines['left'].set_visible(False)\n",
    "ax1_.spines['right'].set_bounds(0,5)\n",
    "ax1_.tick_params(axis='y', labelcolor=colorTwin1)\n",
    "ax1_.set_ylabel(TaskTwinLabel1, color=colorTwin1)\n",
    "##################################################\n",
    "\n",
    "# 1R: All seeds (15) ET and SD progrression\n",
    "\n",
    "gs2= fig.add_gridspec(nrows=1, ncols=1, left=0.6, bottom=0.58, right=0.98, top=.73)\n",
    "ax2= fig.add_subplot(gs2[0])\n",
    "\n",
    "TaskParamToPlot = \"ET\"\n",
    "plot_learning_curve(ax2,all_times_median,goalTime,seed_vector,n_sessions,TaskParamToPlot,color='k')\n",
    "ax2.set_ylabel(TaskParamToPlot)\n",
    "ax2.set_xlabel('Session#')\n",
    "\n",
    "TaskParamToPlot = r'$SD_{ET}$'\n",
    "colorTwin1 = 'Olive'\n",
    "TaskTwinLabel1 = TaskParamToPlot\n",
    "ax2_ = ax2.twinx()\n",
    "plot_learning_curve(ax2_,all_times_std,goalTime,seed_vector,n_sessions,TaskParamToPlot,color=colorTwin1)\n",
    "ax2_.set_ylim([0,5])\n",
    "ax2_.set_yticks([0,2,4])\n",
    "ax2_.set_yticklabels([0,2,4])\n",
    "ax2_.xaxis.set_visible(False)\n",
    "ax2_.spines['bottom'].set_visible(False)\n",
    "ax2_.spines['top'].set_visible(False)\n",
    "ax2_.spines['left'].set_visible(False)\n",
    "ax2_.spines['right'].set_visible(True)\n",
    "ax2_.spines['right'].set_bounds(0,5)\n",
    "ax2_.tick_params(axis='y', labelcolor=colorTwin1)\n",
    "ax2_.set_ylabel(TaskTwinLabel1, color=colorTwin1)\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "\n",
    "# 2: Probabilities trajectories\n",
    "\n",
    "gs2= fig.add_gridspec(nrows=1, ncols=4, left=0.02, bottom=0.3, right=0.98, top=0.45,wspace=0.45,hspace=0.3)\n",
    "axes2= plot_trajectory_distributions(fig, gs2,p_TR_correct,p_TR_error,p_TR_omission,cmapLime,cmapTomato,cmapGray,selected_sessions,alpha=0.5)\n",
    "axes2[-1].set_yticklabels([])\n",
    "axes2[-1].set_ylabel('')\n",
    "for ax in axes2: ax.title.set_fontsize(10)\n",
    "add_legend_trajectory(axes2[-1])\n",
    "\n",
    "##########################################################################################################\n",
    "##########################################################################################################\n",
    "    \n",
    "# 3:Entrance times\n",
    "\n",
    "gs3= fig.add_gridspec(nrows=1, ncols=4, left=0.02, bottom=0.02, right=0.98, top=0.17,wspace=0.45,hspace=0.3)\n",
    "axes3=plot_entrance_times(fig, gs3,sessions,selected_sessions,correct_all,entrance_times,goalTime)\n",
    "\n",
    "axes3[-1].set_yticklabels([])\n",
    "axes3[-1].set_ylabel('')\n",
    "for ax in axes3: ax.title.set_fontsize(10)    \n",
    "add_legend_entrance_times(axes3[-1])\n",
    "\n",
    "    \n",
    "##########################################################################################################    \n",
    "AXES=([ax0,ax1,axes2[0],axes3[0]])\n",
    "OFFX=(.05,)*len(AXES)\n",
    "OFFY=(.03,)*len(AXES)\n",
    "add_panel_caption(axes=AXES, offsetX=OFFX, offsetY=OFFY)     \n",
    "##########################################################################################################\n",
    "fig.savefig(results_folder + '/ALL-AGENTS-modelResults_2Cols-and-Agent-betaStep-' + str(invTstep) + '-invT0-' + str(invT_vect[ind_agent]) + '-alpha-' + str(lr) + '-seedN-' + str(seed_num)  + '.pdf',bbox_inches='tight')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
